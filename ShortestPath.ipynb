{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a70a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b29c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the shortest path from state 0, to state 15\n",
    "#create observation space using numpy array\n",
    "#actions are 0=left, 1=down, 2=right, 3=up\n",
    "#use the appropriate algorithm to learn\n",
    "#print the optimal policy\n",
    "\n",
    "class ShortestPath(Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.observation_space = np.arange(16).reshape(4,4)\n",
    "        self.action_space = Discrete(4)\n",
    "        self.row = 0\n",
    "        self.column = 0\n",
    "        self.state = self.observation_space[self.row][self.column]\n",
    "        \n",
    "    def step(self, action):\n",
    "        reward = -1\n",
    "        \n",
    "        #0 left\n",
    "        if action == 0:\n",
    "            if self.column!=0 :\n",
    "                self.column -= 1  \n",
    "        #1 down\n",
    "        if action == 1:\n",
    "            if self.row!=3:\n",
    "                self.row += 1  \n",
    "        #2 right\n",
    "        if action == 2:\n",
    "            if self.column!=3 :\n",
    "                self.column += 1\n",
    "        #3 up\n",
    "        if action == 3:\n",
    "            if self.row!=0:\n",
    "                self.row -= 1\n",
    "                \n",
    "        new_state = self.observation_space[self.row][self.column]\n",
    "\n",
    "        if new_state==15:\n",
    "            done = True\n",
    "            reward += 30\n",
    "        else:\n",
    "            done = False\n",
    "            \n",
    "        info = {}\n",
    "        \n",
    "        return new_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.row = 0\n",
    "        self.column = 0\n",
    "        self.state = self.observation_space[self.row][self.column]\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9003c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = ShortestPath()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df75243",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((len(env.observation_space)*len(env.observation_space), env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bebc368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(env.observation_space)*len(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9163fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=10000\n",
    "epsilon=1\n",
    "gamma = 1\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25f087ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards= 0\n",
    "rewardsAcrossEpisodes = []\n",
    "#go through episodes\n",
    "#at each episode we reset the environment, and don't forget to set done to False\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    r = 0\n",
    "    \n",
    "    while(True):\n",
    "        #apply epsilon-greedy strategy\n",
    "        num = random.uniform(0,1)\n",
    "        if num < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        #apply the chosen action to the environment\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        #update the value of the q_table according to: old_q = old_q + learning_rate*(reward + discount_factor*max(new_q) - old_q)\n",
    "        q_table[state, action] += learning_rate*(reward + gamma * np.max(q_table[new_state, :]) - q_table[state, action])\n",
    "        \n",
    "        #update state to become next state\n",
    "        #update reward\n",
    "        state = new_state\n",
    "        r += reward\n",
    "        rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            rewardsAcrossEpisodes.append(r)\n",
    "            epsilon = epsilon*0.9\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ceeead2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.74484755, -1.71746132, 24.        , -1.73236612],\n",
       "       [-1.10048599, 25.        , -1.08504825, -1.15455214],\n",
       "       [-0.89483873, -0.09524088, -0.68441279, -0.69      ],\n",
       "       [-0.56198031, -0.33345   , -0.5940931 , -0.5       ],\n",
       "       [-1.19428805, -1.15981208, -0.71166481, -1.25799655],\n",
       "       [-0.77295827, -0.80620844, 26.        , -0.90654347],\n",
       "       [-0.48610608, 27.        , -0.33196689, -0.6145931 ],\n",
       "       [-0.573553  ,  3.16769434, -0.29      , -0.292     ],\n",
       "       [-0.7539    , -0.68294542, -0.67829747, -0.80757146],\n",
       "       [-0.39729   , -0.1141344 ,  0.66616957, -0.571795  ],\n",
       "       [-0.38371   ,  1.177579  , 28.        , -0.6797387 ],\n",
       "       [ 1.13513416, 29.        ,  1.2457    , -0.072     ],\n",
       "       [-0.38      , -0.81608177,  0.39279813, -0.462     ],\n",
       "       [-0.5709459 , -0.2988    ,  5.05397194, -0.21041344],\n",
       "       [ 0.24769503,  0.19      , 15.1293899 ,  0.33555719],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca46971c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract optimal policy\n",
    "optimal_policy = []\n",
    "for state in range(len(env.observation_space)*len(env.observation_space)):\n",
    "    optimal_policy.append(np.argmax(q_table[state, :]))\n",
    "    \n",
    "optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0ff2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
